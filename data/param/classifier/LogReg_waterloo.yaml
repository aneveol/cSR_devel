
LogisticRegression:
  penalty: l2
  dual:    True              # Supposed to be better if n_features > n_examples
                             # However, pegasos solves the primal formulation?
  C:       10                # Where C * n_samples = 1 / lambda
                             # Original uses regularization lambda = 0.0001
                             # corresponding to C = 1000 (disregarding n_samples)
                             # Note that we can get better scores with lower C
  class_weight: balanced

# ~~~~~~~~~~~~ Replication notes ~~~~~~~~~~~~
# 
# Solving the primal formulation yields poor
# performance, which is to be expected when
# n_samples < n_features.
# Pegasos apparently solves the primal.
# 
# Pegasos uses 'Pegasos-style regularization
# and constraints', we use l2.
# 
# Sklearn has no option for sampling training
# samples (ROC used by waterloo), since it is
# using the entire training set.
# We use the exact solution, whereas Pegasos
# is iterative (similar to SGD)
#
# Other details necessary to gain better per-
# formance when using a synthetic seed:
#   - Throw away seed after finding Y
#   - Never add N to training data
#   - Extract features only from abstracts
# Check if this is true also when using real
# seeds!